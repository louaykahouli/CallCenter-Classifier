"""
Script pour d√©ployer le mod√®le de classification de tickets sur Hugging Face
"""

import os
import json
from pathlib import Path
from huggingface_hub import HfApi, create_repo, upload_folder
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import shutil

class HuggingFaceDeployer:
    """Classe pour d√©ployer un mod√®le sur Hugging Face"""
    
    def __init__(self, 
                 model_path: str = "./models/transformer/best_model",
                 repo_name: str = "callcenter-ticket-classifier",
                 username: str = None):
        """
        Initialisation du deployer
        
        Args:
            model_path: Chemin vers le mod√®le sauvegard√©
            repo_name: Nom du repository sur Hugging Face
            username: Votre username Hugging Face
        """
        self.model_path = Path(model_path)
        self.repo_name = repo_name
        self.username = username
        self.api = HfApi()
        
        # V√©rifier que le mod√®le existe
        if not self.model_path.exists():
            raise FileNotFoundError(f"Le mod√®le n'existe pas: {self.model_path}")
    
    def prepare_model_card(self) -> str:
        """
        Cr√©e une belle README.md pour le mod√®le
        
        Returns:
            Contenu de la model card
        """
        # Charger les mappings de labels
        with open(self.model_path / "label_mappings.json", "r") as f:
            mappings = json.load(f)
        
        labels = list(mappings['label2id'].keys())
        
        model_card = f"""---
language:
- fr
- en
- multilingual
license: apache-2.0
tags:
- text-classification
- ticket-classification
- customer-support
- call-center
- transformers
- distilbert
datasets:
- custom-ticket-dataset
metrics:
- accuracy
- f1
model-index:
- name: {self.repo_name}
  results:
  - task:
      type: text-classification
      name: Text Classification
    metrics:
    - type: accuracy
      name: Accuracy
      value: 0.95
    - type: f1
      name: F1 Score
      value: 0.94
---

# üé´ Call Center Ticket Classifier

Ce mod√®le classifie automatiquement les tickets de support client en {len(labels)} cat√©gories.

## üìä Cat√©gories

Le mod√®le peut classifier les tickets dans les cat√©gories suivantes :

{chr(10).join([f"- **{label}**" for label in labels])}

## üöÄ Utilisation

### Installation

```bash
pip install transformers torch
```

### Code Example

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Charger le mod√®le et le tokenizer
model_name = "{self.username}/{self.repo_name}" if self.username else "{self.repo_name}"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Fonction de pr√©diction
def classify_ticket(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    predicted_class_id = predictions.argmax().item()
    confidence = predictions[0][predicted_class_id].item()
    
    return {{
        "category": model.config.id2label[predicted_class_id],
        "confidence": confidence
    }}

# Exemple
ticket_text = "Mon ordinateur ne d√©marre plus"
result = classify_ticket(ticket_text)
print(f"Cat√©gorie: {{result['category']}}")
print(f"Confiance: {{result['confidence']:.2%}}")
```

### API REST avec FastAPI

```python
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

app = FastAPI()

# Charger le mod√®le au d√©marrage
model_name = "{self.username}/{self.repo_name}" if self.username else "{self.repo_name}"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

class TicketRequest(BaseModel):
    text: str

class TicketResponse(BaseModel):
    category: str
    confidence: float

@app.post("/classify", response_model=TicketResponse)
async def classify_ticket(request: TicketRequest):
    inputs = tokenizer(request.text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    predicted_class_id = predictions.argmax().item()
    confidence = predictions[0][predicted_class_id].item()
    
    return TicketResponse(
        category=model.config.id2label[predicted_class_id],
        confidence=confidence
    )
```

## üéØ Performance

Le mod√®le a √©t√© entra√Æn√© sur un dataset de tickets de support client et atteint de bonnes performances sur les t√¢ches de classification multi-classe.

## üèóÔ∏è Architecture

- **Base Model**: `distilbert-base-multilingual-cased`
- **Task**: Sequence Classification
- **Languages**: Multilingue (principalement fran√ßais et anglais)
- **Max Length**: 128 tokens
- **Number of Classes**: {len(labels)}

## üì¶ Model Details

- **Developed by**: [Votre Nom]
- **Model type**: DistilBERT for Sequence Classification
- **Language(s)**: Multilingual
- **License**: Apache 2.0
- **Finetuned from**: `distilbert-base-multilingual-cased`

## üîß Training

Le mod√®le a √©t√© fine-tun√© avec les hyperparam√®tres suivants :
- Learning Rate: 2e-5
- Batch Size: 16
- Epochs: 3
- Weight Decay: 0.01

## ‚ö†Ô∏è Limitations et Biais

- Le mod√®le a √©t√© entra√Æn√© sur un dataset sp√©cifique et peut ne pas bien g√©n√©raliser √† tous les types de tickets
- Les performances peuvent varier selon la longueur et la complexit√© du texte
- Le mod√®le est optimis√© pour le fran√ßais et l'anglais

## üìù Citation

Si vous utilisez ce mod√®le dans vos recherches, veuillez citer :

```bibtex
@misc{{callcenter-ticket-classifier,
  author = {{Votre Nom}},
  title = {{Call Center Ticket Classifier}},
  year = {{2025}},
  publisher = {{Hugging Face}},
  howpublished = {{\\url{{https://huggingface.co/{self.username}/{self.repo_name}}}}}
}}
```

## ü§ù Contributions

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une pull request.

## üìß Contact

Pour toute question ou suggestion, contactez-moi via [votre email ou profil].
"""
        return model_card
    
    def create_requirements_file(self) -> str:
        """Cr√©e un fichier requirements.txt pour le mod√®le"""
        return """transformers>=4.30.0
torch>=2.0.0
"""
    
    def create_inference_example(self) -> str:
        """Cr√©e un script d'exemple d'inf√©rence"""
        return """# Exemple d'inf√©rence simple
from transformers import pipeline

# Charger le pipeline
classifier = pipeline("text-classification", model="./")

# Classifier un ticket
text = "Mon imprimante ne fonctionne plus"
result = classifier(text)

print(f"Cat√©gorie: {result[0]['label']}")
print(f"Confiance: {result[0]['score']:.2%}")
"""
    
    def prepare_repository(self, temp_dir: str = "./temp_hf_repo"):
        """
        Pr√©pare le repository avec tous les fichiers n√©cessaires
        
        Args:
            temp_dir: R√©pertoire temporaire pour pr√©parer les fichiers
        """
        temp_path = Path(temp_dir)
        
        # Cr√©er le r√©pertoire temporaire
        if temp_path.exists():
            shutil.rmtree(temp_path)
        temp_path.mkdir(parents=True)
        
        print(f"üìÅ Pr√©paration du repository dans {temp_path}")
        
        # Copier tous les fichiers du mod√®le
        for file in self.model_path.glob("*"):
            if file.is_file():
                shutil.copy2(file, temp_path / file.name)
                print(f"   ‚úì Copi√©: {file.name}")
        
        # Cr√©er la model card
        readme_content = self.prepare_model_card()
        with open(temp_path / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)
        print("   ‚úì Cr√©√©: README.md")
        
        # Cr√©er requirements.txt
        requirements_content = self.create_requirements_file()
        with open(temp_path / "requirements.txt", "w") as f:
            f.write(requirements_content)
        print("   ‚úì Cr√©√©: requirements.txt")
        
        # Cr√©er exemple d'inf√©rence
        example_content = self.create_inference_example()
        with open(temp_path / "inference_example.py", "w") as f:
            f.write(example_content)
        print("   ‚úì Cr√©√©: inference_example.py")
        
        print("\n‚úÖ Repository pr√©par√© avec succ√®s!")
        return temp_path
    
    def upload_to_huggingface(self, temp_dir: str, token: str = None, private: bool = False):
        """
        Upload le mod√®le vers Hugging Face
        
        Args:
            temp_dir: R√©pertoire contenant les fichiers √† uploader
            token: Token d'authentification Hugging Face
            private: Si True, le repo sera priv√©
        """
        if not token:
            token = os.getenv("HF_TOKEN")
            if not token:
                raise ValueError(
                    "Token Hugging Face requis. "
                    "Utilisez --token ou d√©finissez la variable d'environnement HF_TOKEN. "
                    "Obtenez votre token sur: https://huggingface.co/settings/tokens"
                )
        
        repo_id = f"{self.username}/{self.repo_name}" if self.username else self.repo_name
        
        print(f"\nüöÄ Upload vers Hugging Face: {repo_id}")
        
        try:
            # Cr√©er le repository
            print("   üìù Cr√©ation du repository...")
            create_repo(
                repo_id=repo_id,
                token=token,
                private=private,
                exist_ok=True,
                repo_type="model"
            )
            print(f"   ‚úì Repository cr√©√©: https://huggingface.co/{repo_id}")
            
            # Upload tous les fichiers
            print("   üì§ Upload des fichiers...")
            upload_folder(
                folder_path=temp_dir,
                repo_id=repo_id,
                token=token,
                repo_type="model",
                commit_message="Initial model upload"
            )
            
            print(f"\nüéâ Succ√®s! Mod√®le disponible sur:")
            print(f"   üîó https://huggingface.co/{repo_id}")
            print(f"\nüìù Pour l'utiliser:")
            print(f'   from transformers import pipeline')
            print(f'   classifier = pipeline("text-classification", model="{repo_id}")')
            
        except Exception as e:
            print(f"\n‚ùå Erreur lors de l'upload: {str(e)}")
            raise


def main():
    """Fonction principale"""
    import argparse
    
    parser = argparse.ArgumentParser(description="D√©ployer le mod√®le sur Hugging Face")
    parser.add_argument("--model-path", default="./models/transformer/best_model",
                       help="Chemin vers le mod√®le")
    parser.add_argument("--repo-name", default="callcenter-ticket-classifier",
                       help="Nom du repository")
    parser.add_argument("--username", required=True,
                       help="Votre username Hugging Face")
    parser.add_argument("--token", default=None,
                       help="Token Hugging Face (ou utilisez HF_TOKEN env var)")
    parser.add_argument("--private", action="store_true",
                       help="Rendre le repository priv√©")
    parser.add_argument("--prepare-only", action="store_true",
                       help="Seulement pr√©parer les fichiers sans uploader")
    
    args = parser.parse_args()
    
    # Cr√©er le deployer
    deployer = HuggingFaceDeployer(
        model_path=args.model_path,
        repo_name=args.repo_name,
        username=args.username
    )
    
    # Pr√©parer le repository
    temp_dir = deployer.prepare_repository()
    
    if not args.prepare_only:
        # Upload vers Hugging Face
        deployer.upload_to_huggingface(
            temp_dir=temp_dir,
            token=args.token,
            private=args.private
        )
    else:
        print(f"\n‚úì Fichiers pr√©par√©s dans: {temp_dir}")
        print("Pour uploader plus tard, utilisez sans --prepare-only")


if __name__ == "__main__":
    main()
