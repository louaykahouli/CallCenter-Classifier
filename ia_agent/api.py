"""
API pour l'Agent IA Intelligent
Route les requ√™tes vers TF-IDF ou Transformer selon la complexit√©
Utilise Grok pour g√©n√©rer des r√©ponses intelligentes
"""

from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, validator
import httpx
import logging
import os
import time
import uuid
from typing import Dict, Optional
from intelligent_agent import IntelligentAgent
from cache_manager import CacheManager, ConversationStore
from prometheus_fastapi_instrumentator import Instrumentator

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cl√© API Grok depuis les variables d'environnement
GROK_API_KEY = os.getenv("GROK_API_KEY", "xai-EyqPqZvWyTu8mnQiFCFyYPVuAYdNxPnnjw4z9onvzqrZ5wAjcNkJqWwKx4uc7tY5d68c1njQyeDgJwKx")
GROK_API_URL = "https://api.x.ai/v1/chat/completions"
USE_GROK = os.getenv("USE_GROK", "true").lower() == "true"

# Configuration du cache
CACHE_TTL = int(os.getenv("CACHE_TTL", "3600"))  # 1 heure par d√©faut
CACHE_ENABLED = os.getenv("CACHE_ENABLED", "true").lower() == "true"

# Initialisation de l'application FastAPI
app = FastAPI(
    title="Agent IA Intelligent",
    description="Router intelligent qui choisit le meilleur mod√®le selon la complexit√© du texte",
    version="2.0.0"
)

instrumentator = Instrumentator(
    should_group_status_codes=True,  # groups status codes into 2xx/4xx/5xx
    should_instrument_requests_inprogress=True
)
instrumentator.instrument(app).expose(app, endpoint="/metrics")

# Configuration CORS pour permettre les requ√™tes du frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifier les domaines autoris√©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialisation de l'agent
agent = IntelligentAgent(use_distilbert_for_all=False)

# Initialisation du cache et du stockage
cache_manager = CacheManager(cache_ttl=CACHE_TTL)
conversation_store = ConversationStore(db_path="/app/data/conversations.db")

# Configuration des URLs des mod√®les
TFIDF_API_URL = "http://tfidf-svm:8000/predict"  # URL interne Docker
# Le service Transformer expose /classify (voir Transformer/api/main.py)
TRANSFORMER_API_URL = "http://callcenter:8000/classify"  # URL interne Docker

# Configuration des seuils de routage
COMPLEXITY_THRESHOLD = 35  # Score < 35 ‚Üí TF-IDF, Score >= 35 ‚Üí Transformer


async def generate_grok_response(
    input_text: str,
    prediction: str,
    probabilities: Dict[str, float],
    model_used: str,
    complexity_score: int,
    complexity_level: str
) -> str:
    """
    G√©n√®re une r√©ponse intelligente en utilisant l'API Grok de xAI
    
    Args:
        input_text: Le texte d'entr√©e
        prediction: La cat√©gorie pr√©dite
        probabilities: Les probabilit√©s pour chaque cat√©gorie
        model_used: Le mod√®le utilis√© (tfidf ou transformer)
        complexity_score: Le score de complexit√©
        complexity_level: Le niveau de complexit√©
        
    Returns:
        Une r√©ponse g√©n√©r√©e par Grok en langage naturel
    """
    if not USE_GROK or not GROK_API_KEY:
        logger.warning("Grok d√©sactiv√© ou pas de cl√© API, utilisation du fallback")
        return generate_fallback_response(
            input_text, prediction, probabilities, 
            model_used, complexity_score, complexity_level
        )
    
    try:
        # Pr√©parer le contexte pour Grok
        top_predictions = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:3]
        confidence = top_predictions[0][1] * 100
        
        # Cr√©er le prompt pour Grok
        prompt = f"""Tu es un assistant IA intelligent pour un centre d'appels IT. 

Un ticket vient d'√™tre analys√© avec les r√©sultats suivants:

TICKET: "{input_text}"

R√âSULTATS DE L'ANALYSE:
- Cat√©gorie pr√©dite: {prediction}
- Confiance: {confidence:.1f}%
- Mod√®le utilis√©: {"TF-IDF/SVM (rapide)" if model_used == "tfidf" else "Transformer (pr√©cis)"}
- Score de complexit√©: {complexity_score}/100 ({complexity_level})

TOP 3 PR√âDICTIONS:
{chr(10).join([f"- {cat}: {prob*100:.1f}%" for cat, prob in top_predictions])}

G√âN√àRE une r√©ponse professionnelle et utile pour l'utilisateur qui contient:
1. Une confirmation que tu as compris sa demande
2. La cat√©gorie identifi√©e et pourquoi
3. Une recommandation concr√®te ou prochaine √©tape
4. Un ton sympathique et rassurant

R√©ponds en fran√ßais, en 3-4 phrases maximum, format texte brut (pas de markdown)."""

        # Appeler l'API Grok
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.post(
                GROK_API_URL,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {GROK_API_KEY}"
                },
                json={
                    "messages": [
                        {
                            "role": "system",
                            "content": "Tu es un assistant IA professionnel pour un centre d'appels IT. R√©ponds de mani√®re claire, concise et utile."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "model": "grok-beta",
                    "stream": False,
                    "temperature": 0.7
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                grok_response = result['choices'][0]['message']['content']
                logger.info("R√©ponse Grok g√©n√©r√©e avec succ√®s")
                return grok_response.strip()
            else:
                logger.error(f"Erreur API Grok: {response.status_code}")
                return generate_fallback_response(
                    input_text, prediction, probabilities,
                    model_used, complexity_score, complexity_level
                )
    
    except Exception as e:
        logger.error(f"Erreur lors de l'appel √† Grok: {str(e)}")
        return generate_fallback_response(
            input_text, prediction, probabilities,
            model_used, complexity_score, complexity_level
        )


async def generate_conversation_title(input_text: str, prediction: str) -> str:
    """
    G√©n√®re un titre court et significatif pour la conversation avec Grok
    
    Args:
        input_text: Le premier message de la conversation
        prediction: La cat√©gorie pr√©dite
        
    Returns:
        Un titre court (max 50 caract√®res)
    """
    if not USE_GROK or not GROK_API_KEY:
        # Fallback : utiliser les 50 premiers caract√®res
        title = input_text[:47] + '...' if len(input_text) > 50 else input_text
        return title.capitalize()
    
    try:
        # Cr√©er le prompt pour Grok
        prompt = f"""G√©n√®re un titre court et descriptif (maximum 40 caract√®res) pour cette conversation :

MESSAGE: "{input_text}"
CAT√âGORIE: {prediction}

Le titre doit :
- √ätre court et explicite (max 40 caract√®res)
- R√©sumer l'essentiel de la demande
- Ne pas inclure d'√©moji (sera ajout√© automatiquement)
- Commencer par une majuscule

R√©ponds UNIQUEMENT avec le titre, rien d'autre."""

        # Appeler l'API Grok
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                GROK_API_URL,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {GROK_API_KEY}"
                },
                json={
                    "messages": [
                        {
                            "role": "system",
                            "content": "Tu g√©n√®res des titres courts et descriptifs pour des conversations. R√©ponds uniquement avec le titre, sans guillemets ni ponctuation finale."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "model": "grok-beta",
                    "stream": False,
                    "temperature": 0.5,
                    "max_tokens": 20
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                title = result['choices'][0]['message']['content'].strip()
                # Nettoyer les guillemets si pr√©sents
                title = title.strip('"').strip("'").strip()
                # Limiter √† 50 caract√®res
                if len(title) > 50:
                    title = title[:47] + "..."
                logger.info(f"Titre Grok g√©n√©r√©: {title}")
                return title
            else:
                logger.error(f"Erreur API Grok pour titre: {response.status_code}")
                # Fallback
                title = input_text[:47] + '...' if len(input_text) > 50 else input_text
                return title.capitalize()
    
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du titre: {str(e)}")
        # Fallback
        title = input_text[:47] + '...' if len(input_text) > 50 else input_text
        return title.capitalize()


def generate_fallback_response(
    input_text: str,
    prediction: str,
    probabilities: Dict[str, float],
    model_used: str,
    complexity_score: int,
    complexity_level: str
) -> str:
    """
    G√©n√®re une r√©ponse simple sans Grok (fallback)
    """
    top_predictions = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:3]
    confidence = top_predictions[0][1] * 100
    
    category_messages = {
        "Hardware": "un probl√®me mat√©riel",
        "Access": "une demande d'acc√®s ou de permissions",
        "HR Support": "une question RH",
        "Administrative rights": "une demande de droits administratifs",
        "Storage": "un probl√®me de stockage",
        "Purchase": "une demande d'achat",
        "Internal Project": "une question de projet interne",
        "Miscellaneous": "une demande diverse"
    }
    
    category_desc = category_messages.get(prediction, "une demande")
    
    response = f"""J'ai analys√© votre demande et identifi√© {category_desc} (cat√©gorie: {prediction}).

Ma confiance dans cette classification est de {confidence:.1f}%.

Mod√®le utilis√©: {"TF-IDF/SVM (analyse rapide)" if model_used == "tfidf" else "Transformer (analyse approfondie)"}.

Votre demande a √©t√© correctement cat√©goris√©e et sera trait√©e par le service appropri√©."""
    
    return response


class TextRequest(BaseModel):
    """Sch√©ma de la requ√™te"""
    text: str
    force_model: Optional[str] = None  # 'tfidf' ou 'transformer' pour forcer un mod√®le
    session_id: Optional[str] = None  # ID de session pour le tracking
    conversation_title: Optional[str] = None  # Titre descriptif de la conversation
    
    @validator('text')
    def text_must_not_be_empty(cls, v):
        """Valider que le texte n'est pas vide"""
        if not v or not v.strip():
            raise ValueError('Le texte ne peut pas √™tre vide')
        return v


class PredictionResponse(BaseModel):
    """Sch√©ma de la r√©ponse"""
    input: str
    prediction: str
    probabilities: Dict[str, float]
    model_used: str
    complexity_analysis: Dict
    reasoning: str
    generated_response: str
    session_id: str
    cache_hit: bool = False  # Indique si la r√©ponse vient du cache
    prediction: str
    probabilities: Dict[str, float]
    model_used: str
    complexity_analysis: Dict
    reasoning: str
    generated_response: str  # Nouvelle r√©ponse g√©n√©r√©e en langage naturel


@app.get("/")
async def root():
    """Point d'entr√©e de l'API"""
    return {
        "service": "Agent IA Intelligent",
        "version": "1.0.0",
        "description": "Router intelligent vers TF-IDF ou Transformer",
        "endpoints": {
            "/predict": "Pr√©diction avec routage intelligent",
            "/analyze": "Analyse de complexit√© uniquement",
            "/health": "V√©rification de l'√©tat",
            "/stats": "Statistiques d'utilisation"
        }
    }


@app.get("/health")
async def health_check():
    """V√©rification de l'√©tat de l'API"""
    # Tester la connexion aux deux mod√®les
    tfidf_status = "unknown"
    transformer_status = "unknown"
    
    async with httpx.AsyncClient(timeout=5.0) as client:
        # Test TF-IDF
        try:
            response = await client.get("http://tfidf-svm:8000/health")
            tfidf_status = "healthy" if response.status_code == 200 else "unhealthy"
        except Exception as e:
            tfidf_status = f"unreachable: {str(e)}"
        
        # Test Transformer
        try:
            response = await client.get("http://callcenter:8000/health")
            transformer_status = "healthy" if response.status_code == 200 else "unhealthy"
        except Exception as e:
            transformer_status = f"unreachable: {str(e)}"
    
    return {
        "status": "healthy",
        "agent": "operational",
        "models": {
            "tfidf": tfidf_status,
            "transformer": transformer_status
        },
        "threshold": COMPLEXITY_THRESHOLD
    }


@app.post("/analyze")
async def analyze_complexity(request: TextRequest):
    """
    Analyse la complexit√© d'un texte sans faire de pr√©diction
    """
    try:
        # Analyser la complexit√©
        routing_result = agent.route(request.text)
        
        # D√©terminer quel mod√®le serait utilis√©
        complexity_score = routing_result['complexity_score']
        recommended_model = "tfidf" if complexity_score < COMPLEXITY_THRESHOLD else "transformer"
        
        return {
            "text": request.text[:100] + "..." if len(request.text) > 100 else request.text,
            "complexity_score": complexity_score,
            "complexity_level": routing_result['complexity_level'],
            "recommended_model": recommended_model,
            "details": routing_result['details'],
            "reasoning": routing_result['reasoning']
        }
    
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur d'analyse: {str(e)}")


@app.post("/predict", response_model=PredictionResponse)
async def predict_with_routing(request: TextRequest):
    """
    Pr√©dit la cat√©gorie d'un ticket en choisissant automatiquement le meilleur mod√®le
    selon la complexit√© du texte. Utilise le cache pour am√©liorer les performances.
    """
    start_time = time.time()
    cache_hit = False
    
    # G√©n√©rer ou utiliser le session_id
    session_id = request.session_id or str(uuid.uuid4())
    
    try:
        # 1. V√©rifier le cache si activ√©
        if CACHE_ENABLED and not request.force_model:
            cached_result = cache_manager.get(request.text)
            if cached_result:
                logger.info(f"‚úÖ Cache HIT pour session {session_id[:8]}...")
                cached_result['session_id'] = session_id
                cached_result['cache_hit'] = True
                
                # Sauvegarder quand m√™me la conversation en DB (pour l'historique)
                try:
                    # G√©n√©rer un titre si c'est une nouvelle session
                    conversation_title = request.conversation_title
                    if not conversation_title or conversation_title.strip() == "":
                        if len(request.text) > 40:
                            conversation_title = request.text[:37] + "..."
                        else:
                            conversation_title = request.text
                        conversation_title = conversation_title.capitalize()
                    
                    conversation_store.save_conversation(
                        session_id=session_id,
                        input_text=request.text,
                        prediction=cached_result['prediction'],
                        model_used=cached_result['model_used'],
                        complexity_score=cached_result['complexity_analysis']['score'],
                        complexity_level=cached_result['complexity_analysis']['level'],
                        probabilities=cached_result['probabilities'],
                        response_time=0.0,  # Temps de r√©ponse du cache n√©gligeable
                        generated_response=cached_result['generated_response'],
                        conversation_title=conversation_title
                    )
                    logger.info(f"üíæ Conversation sauvegard√©e (cache hit)")
                except Exception as db_error:
                    logger.error(f"Erreur DB lors du cache hit: {db_error}")
                
                return cached_result
        
        # 2. Analyser la complexit√©
        routing_result = agent.route(request.text)
        complexity_score = routing_result['complexity_score']
        
        # 3. D√©terminer le mod√®le √† utiliser
        if request.force_model:
            # Si un mod√®le est forc√©
            model_to_use = request.force_model.lower()
            logger.info(f"Mod√®le forc√©: {model_to_use}")
        else:
            # Routage intelligent bas√© sur la complexit√©
            model_to_use = "tfidf" if complexity_score < COMPLEXITY_THRESHOLD else "transformer"
            logger.info(f"Routage automatique: complexit√©={complexity_score} ‚Üí {model_to_use}")
        
        # 4. Appeler le mod√®le appropri√©
        prediction_result = await _call_model(model_to_use, request.text)
        
        prediction = prediction_result.get("prediction", prediction_result.get("predicted_category"))
        probabilities = prediction_result.get("probabilities", {})
        
        # 5. G√©n√©rer une r√©ponse intelligente avec Grok
        generated_response = await generate_grok_response(
            input_text=request.text,
            prediction=prediction,
            probabilities=probabilities,
            model_used=model_to_use,
            complexity_score=complexity_score,
            complexity_level=routing_result['complexity_level']
        )
        
        # 5.5. G√©n√©rer un titre intelligent si pas fourni et c'est une nouvelle conversation
        conversation_title = request.conversation_title
        if not conversation_title or conversation_title.strip() == "":
            # G√©n√©rer un titre simple mais descriptif (sans appeler Grok pour √©viter les erreurs)
            # Format: r√©sum√© du texte + cat√©gorie
            if len(request.text) > 40:
                conversation_title = request.text[:37] + "..."
            else:
                conversation_title = request.text
            # Capitaliser la premi√®re lettre
            conversation_title = conversation_title.capitalize()
            logger.info(f"üìù Titre g√©n√©r√©: {conversation_title}")
        else:
            logger.info(f"üìù Titre fourni: {conversation_title}")
        
        # 6. Calculer le temps de r√©ponse
        response_time = time.time() - start_time
        
        # 7. Construire la r√©ponse compl√®te
        response = {
            "input": request.text,
            "prediction": prediction,
            "probabilities": probabilities,
            "model_used": model_to_use,
            "complexity_analysis": {
                "score": complexity_score,
                "level": routing_result['complexity_level'],
                "details": routing_result['details']
            },
            "reasoning": routing_result['reasoning'] + f" ‚Üí Mod√®le utilis√©: {model_to_use.upper()}",
            "generated_response": generated_response,
            "session_id": session_id,
            "cache_hit": False
        }
        
        # 8. Sauvegarder dans le cache (seulement si pas forc√©)
        if CACHE_ENABLED and not request.force_model:
            cache_manager.set(request.text, response)
            logger.info(f"üíæ R√©ponse mise en cache")
        
        # 9. Sauvegarder la conversation dans la base de donn√©es
        try:
            conversation_store.save_conversation(
                session_id=session_id,
                input_text=request.text,
                prediction=prediction,
                model_used=model_to_use,
                complexity_score=complexity_score,
                complexity_level=routing_result['complexity_level'],
                probabilities=probabilities,
                response_time=response_time,
                generated_response=generated_response,
                conversation_title=conversation_title  # Titre g√©n√©r√© ou fourni
            )
        except Exception as db_error:
            logger.error(f"Erreur lors de la sauvegarde en DB: {db_error}")
            # Ne pas faire √©chouer la requ√™te si la DB pose probl√®me
        
        return response
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©diction: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction: {str(e)}")


async def _call_model(model_name: str, text: str) -> Dict:
    """
    Appelle l'API du mod√®le sp√©cifi√© et normalise la r√©ponse

    Retourne un dict standardis√© avec au minimum:
      - prediction: str
      - probabilities: Dict[str, float]
      - raw: la r√©ponse brute (si besoin)
    """
    # Choisir l'URL appropri√©e
    if model_name == "tfidf":
        url = TFIDF_API_URL
        payload = {"text": text}
    elif model_name == "transformer":
        url = TRANSFORMER_API_URL
        payload = {"text": text}
    else:
        raise HTTPException(status_code=400, detail=f"Mod√®le inconnu: {model_name}")

    async with httpx.AsyncClient(timeout=30.0) as client:
        try:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            data = response.json()

            # Normaliser selon la source
            if model_name == "tfidf":
                # tfidf API renvoie: {input, prediction, probabilities}
                return {
                    "prediction": data.get("prediction"),
                    "probabilities": data.get("probabilities", {}),
                    "raw": data
                }
            else:
                # transformer API (callcenter) renvoie: {text, predicted_category, confidence, all_predictions}
                return {
                    "prediction": data.get("predicted_category") or data.get("prediction"),
                    "probabilities": data.get("all_predictions") or data.get("probabilities") or {},
                    "confidence": data.get("confidence"),
                    "raw": data
                }

        except httpx.TimeoutException:
            logger.error(f"Timeout lors de l'appel √† {model_name}")
            raise HTTPException(status_code=504, detail=f"Le mod√®le {model_name} n'a pas r√©pondu √† temps")

        except httpx.HTTPStatusError as e:
            body = e.response.text if e.response is not None else str(e)
            logger.error(f"Erreur HTTP {e.response.status_code} du mod√®le {model_name}: {body}")
            raise HTTPException(status_code=e.response.status_code, detail=f"Erreur du mod√®le {model_name}: {body}")

        except Exception as e:
            logger.error(f"Erreur lors de l'appel √† {model_name}: {str(e)}")
            raise HTTPException(status_code=503, detail=f"Le mod√®le {model_name} est inaccessible: {str(e)}")


@app.get("/stats")
async def get_statistics():
    """
    Retourne les statistiques d'utilisation de l'agent incluant cache et conversations
    """
    stats = agent.get_stats()
    cache_stats = cache_manager.get_stats()
    db_stats = conversation_store.get_global_stats(days=7)
    
    return {
        "agent_statistics": stats,
        "cache_statistics": cache_stats,
        "conversation_statistics": db_stats,
        "configuration": {
            "complexity_threshold": COMPLEXITY_THRESHOLD,
            "cache_enabled": CACHE_ENABLED,
            "cache_ttl": CACHE_TTL,
            "routing_strategy": f"TF-IDF (< {COMPLEXITY_THRESHOLD}) / Transformer (‚â• {COMPLEXITY_THRESHOLD})"
        }
    }


@app.get("/history/{session_id}")
async def get_session_history(session_id: str, limit: int = 50):
    """
    R√©cup√®re l'historique des conversations d'une session
    
    Args:
        session_id: ID de la session
        limit: Nombre maximum de conversations √† retourner
    """
    try:
        history = conversation_store.get_session_history(session_id, limit)
        return {
            "session_id": session_id,
            "count": len(history),
            "conversations": history
        }
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration de l'historique: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/cache/clear")
async def clear_cache():
    """
    Vide compl√®tement le cache
    """
    try:
        count = cache_manager.clear()
        return {
            "message": "Cache vid√© avec succ√®s",
            "entries_cleared": count
        }
    except Exception as e:
        logger.error(f"Erreur lors du vidage du cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/cache/cleanup")
async def cleanup_cache():
    """
    Nettoie les entr√©es expir√©es du cache
    """
    try:
        count = cache_manager.cleanup_expired()
        return {
            "message": "Nettoyage effectu√©",
            "entries_removed": count
        }
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage du cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/cache/stats")
async def get_cache_stats():
    """
    R√©cup√®re les statistiques d√©taill√©es du cache
    """
    try:
        stats = cache_manager.get_stats()
        return stats
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des stats du cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/config/threshold")
async def update_threshold(new_threshold: int):
    """
    Met √† jour le seuil de complexit√© pour le routage
    
    Args:
        new_threshold: Nouveau seuil (0-100)
    """
    global COMPLEXITY_THRESHOLD
    
    if not 0 <= new_threshold <= 100:
        raise HTTPException(
            status_code=400,
            detail="Le seuil doit √™tre entre 0 et 100"
        )
    
    old_threshold = COMPLEXITY_THRESHOLD
    COMPLEXITY_THRESHOLD = new_threshold
    
    return {
        "message": "Seuil mis √† jour",
        "old_threshold": old_threshold,
        "new_threshold": new_threshold,
        "routing_strategy": f"TF-IDF (< {new_threshold}) / Transformer (‚â• {new_threshold})"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
